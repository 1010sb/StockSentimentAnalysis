{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7f1771",
   "metadata": {},
   "source": [
    "## StockTwits Scraper\n",
    "#### Efficiently Collecting Tweets Based on MaxID and Stock Symbol and Date\n",
    "\n",
    "- Facilitates the collection of tweets from the StockTwits platform.\n",
    "- Requires input of a maxID number and a stock symbol for efficient data retrieval.\n",
    "- Gathers tweets starting from a specific point in time up to the present date.\n",
    "- Utilizes the maxID obtained from the latest tweets scrape to retrieve historical tweets.\n",
    "- Simplifies the process of acquiring relevant stock-related data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d3c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time, json, os, traceback\n",
    "from json import JSONDecodeError\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "from collections import deque\n",
    "\n",
    "class StockTwitsAPIScraper:\n",
    "    def __init__(self, symbol, date, maxId):\n",
    "        self.symbol = symbol\n",
    "        self.link = \"https://api.stocktwits.com/api/2/streams/symbol/{}.json?\".format(symbol)\n",
    "        self.targetDate = date\n",
    "        self.tweets = []\n",
    "        self.reqeustQueue = deque()\n",
    "        self.maxId = maxId\n",
    "        self.initDir()\n",
    "\n",
    "    def setLimits(self, size, duration):\n",
    "        self.size = size\n",
    "        self.duration = duration\n",
    "        self.requestInterval = duration // size + 1 if duration % size else duration // size\n",
    "\n",
    "    # create directories if they don't exist\n",
    "    def initDir(self):\n",
    "        if not os.path.isdir(\"stocks\"):\n",
    "            os.mkdir(\"stocks\")\n",
    "        if not os.path.isdir(\"stocks/{}\".format(self.symbol)):\n",
    "            os.mkdir(\"stocks/{}\".format(self.symbol))\n",
    "\n",
    "    # write tweets we get and the ID of the last tweet in case the system breaks down\n",
    "    def writeJson(self):\n",
    "        if self.tweets:\n",
    "            self.maxId = self.tweets[-1][\"id\"]\n",
    "            fileName = \"stocks/{}/{}.json\".format(self.symbol, self.maxId)\n",
    "            with open(fileName, \"w\") as f:\n",
    "                json.dump(self.tweets, f)\n",
    "    \n",
    "    def getCurrentUrl(self):\n",
    "        return self.link + \"max={}\".format(self.maxId)\n",
    "\n",
    "    # request manager\n",
    "    # can't exceed 200 requests within an hour\n",
    "    def requestManager(self):\n",
    "        if len(self.reqeustQueue) == self.size:\n",
    "            now = datetime.now()\n",
    "            firstRequest = self.reqeustQueue.popleft()\n",
    "            if now < firstRequest + timedelta(seconds=self.duration):\n",
    "                timeDiff = firstRequest - now\n",
    "                waitTime = timeDiff.total_seconds() + 1 + self.duration                \n",
    "                print(\"Reached the request limit, waiting for {} seconds.\".format(waitTime))\n",
    "                sleep(waitTime)\n",
    "\n",
    "    def getMessages(self, url):\n",
    "        self.requestManager()\n",
    "\n",
    "        response = requests.get(url)\n",
    "        self.reqeustQueue.append(datetime.now())\n",
    "        try:\n",
    "            data = json.loads(response.text)\n",
    "        except JSONDecodeError:\n",
    "            if \"Bad Gateway\" in response.text:\n",
    "                print(\"Just a Bad Gateway, waiting for 1 minute.\")\n",
    "                sleep(60)\n",
    "                return True\n",
    "            print(len(self.reqeustQueue))\n",
    "            print(self.reqeustQueue[0], datetime.now())\n",
    "            print(url)\n",
    "            print(response.text)\n",
    "            print(traceback.format_exc())\n",
    "            raise Exception(\"Something went wrong with the response.\")\n",
    "        if data and data[\"response\"][\"status\"] == 200:\n",
    "            data[\"cursor\"][\"max\"]\n",
    "            for m in data[\"messages\"]:\n",
    "                record = {}            \n",
    "                createdAt = datetime.strptime(m[\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                if createdAt < self.targetDate:\n",
    "                    return False\n",
    "                record[\"id\"] = m[\"id\"]\n",
    "                record[\"text\"] = m[\"body\"]\n",
    "                record[\"time\"] = createdAt.timestamp()\n",
    "                record[\"sentiment\"] = m[\"entities\"][\"sentiment\"][\"basic\"] if m[\"entities\"][\"sentiment\"] else \"\"\n",
    "                self.tweets.append(record)\n",
    "                print(\"Scraped tweet with ID:\", record[\"id\"])  # Print progress for each tweet\n",
    "        else:\n",
    "            print(response.text)        \n",
    "        return True\n",
    "\n",
    "    def getTweetsAndWriteToFile(self):        \n",
    "        if not self.getMessages(self.getCurrentUrl()):\n",
    "            return False\n",
    "        self.writeJson()\n",
    "        print(\"Scraped {} tweets starting from {}.\".format(len(self.tweets), self.maxId))\n",
    "        self.tweets.clear()\n",
    "        sleep(self.requestInterval)\n",
    "        return True\n",
    "\n",
    "    def scrapTweets(self):        \n",
    "        try:\n",
    "            doScrap = True\n",
    "            while doScrap:\n",
    "                doScrap = self.getTweetsAndWriteToFile()\n",
    "        except Exception:\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "symbol = input(\"Enter stock symbol: \")\n",
    "print(\"This scraper scrapes tweets backward.\\n\\\n",
    "The ID you put in belongs to the most recent tweet you're going to scrape.\\n\\\n",
    "And the scraper will keep going backward to scrape older tweets.\")\n",
    "maxId = input(\"Enter the starting tweet ID: \")\n",
    "targetDate = input(\"Enter the earliest date (mmddyyyy): \")\n",
    "print(\"You can only send 200 requests to StockTwits in an hour.\")\n",
    "requestLimit = input(\"Enter the limit of the number of requests within an hour: \")\n",
    "\n",
    "scraper = StockTwitsAPIScraper(symbol, datetime.strptime(targetDate, \"%m%d%Y\"), int(maxId))\n",
    "scraper.setLimits(int(requestLimit), 3600)\n",
    "scraper.scrapTweets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43c7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b592a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31938d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
